{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c65ff1-ba49-41c7-8f89-92c2ff9df852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore, linregress, pearsonr\n",
    "from matplotlib.colors import LogNorm\n",
    "import os\n",
    "import pycwt as cwt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca07864-8260-41ce-9d4f-595fcbd42500",
   "metadata": {},
   "outputs": [],
   "source": [
    "FS = 4 # E4 sampling rate\n",
    "\n",
    "SAVE_CWT_FOLDER = '/media/bayesian-posterior/sdc/sensecode_data/hr/cwt_hr_all_freq/'\n",
    "\n",
    "DATA_FOLDER_PATH = '/media/bayesian-posterior/sdc/sensecode_data/hr/'\n",
    "DATA_FOLDER = os.fsencode(DATA_FOLDER_PATH)\n",
    "\n",
    "# DAYS_ARRAY = np.asarray([0.5, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31])\n",
    "# FREQ_ARRAY = 1 / (DAYS_ARRAY * 24) # for morlet\n",
    "# FREQ_NAMES = [\"{:.1f}\".format(day)+' Day(s)' for day in DAYS_ARRAY]\n",
    "\n",
    "PANDAS_RESAMPLE_RATE = \"{:.2f}\".format(1/FS)+'S' # find missing samples (datetime)\n",
    "# PANDAS_RESAMPLE_RATE = 'T'\n",
    "\n",
    "# wavelet and filter variables                    \n",
    "mother = cwt.Morlet(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fd477a-8b7c-473b-803f-1bb81cb47fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "HDRS_17_CUTOFF = 20\n",
    "HDRS_24_CUTOFF = 10\n",
    "RCI_HDRS_17 = 6\n",
    "\n",
    "HDRS_variant = 'hamd_17_score'\n",
    "\n",
    "LABEL_FOLDER_PATH = 'labels/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9d4982-7eca-4271-8a45-e0794f07bdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_signal(df_eda,\n",
    "                fs,\n",
    "                label,\n",
    "                ylim = None):\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    t = df_eda.index.to_series()\n",
    "    x = df_eda[label]\n",
    "    xtick_freq = int(fs*60*60*24*7)\n",
    "    \n",
    "    plt.plot(range(len(t)), x, label = label)\n",
    "    plt.legend(loc=1)\n",
    "    plt.xticks(range(len(t))[::xtick_freq], t[::xtick_freq], rotation='vertical')\n",
    "\n",
    "    if ylim is not None:\n",
    "        plt.ylim(ylim)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('EDA [uS]')\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0e5d9b-b2dc-4368-b80a-ced6444780a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_artifacts_and_interpolate(df_eda, \n",
    "                     fs, \n",
    "                     pandas_resample_rate,\n",
    "                                     interpolate_method = 'time',\n",
    "                                     z_score = True):\n",
    "    \n",
    "    df_eda_without_artifacts = df_eda.resample(pandas_resample_rate).mean()\n",
    "    \n",
    "    # df_eda_without_artifacts['hour'] = df_eda_without_artifacts.index.to_series().apply(lambda x : x.hour)    \n",
    "    # df_eda_without_artifacts['eda'] = df_eda_without_artifacts.groupby('hour', group_keys=False)['eda'].apply(lambda x: x.fillna(x.mean()))\n",
    "    \n",
    "    df_eda_without_artifacts.fillna(df_eda_without_artifacts['hr'].median(), inplace=True)\n",
    "    \n",
    "    if z_score:\n",
    "        print('z-scored')\n",
    "        df_eda_without_artifacts['hr'] = zscore(df_eda_without_artifacts['hr'])\n",
    "        \n",
    "\n",
    "    return df_eda_without_artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba621878-b510-40f1-92d0-fba07be4f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fir(eda: np.ndarray,\n",
    "              fs,\n",
    "              cutoff,\n",
    "              filter_type, # bandpass, lowpass, highpass, bandstop\n",
    "              transition_band,\n",
    "              window) -> np.ndarray:\n",
    "    \n",
    "    if window == 'hann' or window == 'hamming' or window == 'bartlett':\n",
    "        M = int(4 * fs / transition_band)\n",
    "    elif window == 'blackman':\n",
    "        M = int(6 * fs / transition_band)\n",
    "    else:\n",
    "        raise ValueError('Length estimation for this window not implemented')\n",
    "        \n",
    "    # print('Using ' + window + ' window for ' + filter_type + ' FIR filter.')\n",
    "    h = signal.firwin(numtaps = M, \n",
    "                      cutoff = cutoff,\n",
    "                      fs = fs,\n",
    "                      pass_zero = filter_type,\n",
    "                      window = window)\n",
    "    \n",
    "    eda = np.squeeze(eda)\n",
    "    return signal.lfilter(h, [1.0], eda) \n",
    "    # return signal.filtfilt(h, [1.0], eda) # avoid phase shift of single filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9718f303-95a4-41ca-b84f-db8f838a0254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def morlet_wavelet(eda: np.ndarray, \n",
    "                   fs, \n",
    "                   freq_arr, \n",
    "                   w = 6):\n",
    "    \n",
    "    widths = w * fs / (2 * freq_arr * np.pi)\n",
    "    cwtm = signal.cwt(eda, signal.morlet2, widths, w = w)\n",
    "    \n",
    "    return np.abs(cwtm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f6b3a9-d0fd-4422-8491-9293b437f974",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_peaks_dict = {}\n",
    "all_power_dict = {}\n",
    "\n",
    "sig_peaks_dict = {}\n",
    "sig_power_dict = {}\n",
    "\n",
    "for file in os.listdir(DATA_FOLDER):\n",
    "    \n",
    "    filename = os.fsdecode(file)\n",
    "    \n",
    "    if filename.endswith(\"worn_left.h5\"):\n",
    "        \n",
    "        subject = filename.split('_')[0]\n",
    "        eda_filepath = DATA_FOLDER_PATH+filename\n",
    "        \n",
    "        print(eda_filepath, subject)\n",
    "        \n",
    "        df_eda = pd.read_hdf(eda_filepath)\n",
    "\n",
    "        # plot_signal(df_eda = df_eda, label = 'eda', fs = FS)\n",
    "        df_eda_without_artifacts = remove_artifacts_and_interpolate(df_eda = df_eda, \n",
    "                                                                    fs = FS, \n",
    "                                                                    pandas_resample_rate = PANDAS_RESAMPLE_RATE, \n",
    "                                                                    z_score = True)\n",
    "        \n",
    "        # delta = df_eda_without_artifacts.index[-1] - df_eda_without_artifacts.index[0]\n",
    "        # n = delta.total_seconds()/3600/2\n",
    "        # freqs = np.append(np.arange(20, 28, 1), np.arange(28, 3*24, 4))\n",
    "        # freqs = np.append(freqs, np.arange(3*24, 8*24, 6))\n",
    "        # freqs = np.append(freqs, np.arange(8*24, int(n), 12))\n",
    "        freqs = np.append(np.arange(23, 25, 1), np.arange(6*24, 8*24, 24))\n",
    "        freqs = (1/freqs)\n",
    "        # freqs = FREQ_ARRAY\n",
    "        \n",
    "        # wavelet\n",
    "        y = df_eda_without_artifacts.resample('T').median()['hr'].to_numpy()\n",
    "        dt = 1/60\n",
    "        alpha, _, _ = cwt.ar1(y) # lag 1 autocorrelation for significance\n",
    "        wave, scales, freqs, coi, fft, fftfreqs = cwt.cwt(signal = y, dt = dt, wavelet = mother, freqs = freqs)\n",
    "        power = np.abs(wave) ** 2 # [len(freq), t]\n",
    "        fft_power = np.abs(fft) ** 2\n",
    "        period = 1 / freqs\n",
    "        \n",
    "        # save downsampled cwt of each subject\n",
    "        subject_cwt_df = pd.DataFrame(power.T, columns = [str(p) for p in period])\n",
    "        cwt_h5_name = SAVE_CWT_FOLDER + subject + '_cwt.h5'\n",
    "        subject_cwt_df.index = df_eda_without_artifacts.resample('T').median().index\n",
    "        subject_cwt_df.to_hdf(cwt_h5_name, key='df', mode='w')\n",
    "        \n",
    "        glbl_power = power.mean(axis=1) # len(freqs)\n",
    "        dof = y.size - scales  # Correction for padding at edges\n",
    "        var = y.std()**2\n",
    "        glbl_signif, tmp = cwt.significance(var, dt, scales, 1, alpha, significance_level=0.95, dof=dof, wavelet=mother)\n",
    "        # signif, fft_theor = cwt.significance(1.0, dt, scales, 0, alpha, significance_level=0.99, wavelet=mother)\n",
    "        \n",
    "        # Find peaks that are significant\n",
    "        xpeaks = []; powers = []\n",
    "        ind_peaks = signal.find_peaks(var * glbl_power)[0]\n",
    "        for i in ind_peaks:\n",
    "            peak = [var * glbl_power > glbl_signif][0][i]\n",
    "            if peak:\n",
    "                if period[i] not in xpeaks:\n",
    "                    xpeaks.append(period[i])\n",
    "                    powers.append([var * glbl_power][0][i])\n",
    "\n",
    "        # keep only stongest peak if there is a peak within +/- 33% of another peak\n",
    "        # xpeaks = np.array(xpeaks)  \n",
    "        # new_xpeaks = {}\n",
    "        # for peak in xpeaks:\n",
    "        #     ints2 = np.where(np.logical_and(xpeaks>=peak-0.33*peak, xpeaks<=peak+0.33*peak))\n",
    "        #     # is the peak in another peaks BP filter?\n",
    "        #     other = [i for i,p in enumerate(xpeaks) if peak >= p - 0.33*p and peak <= p + 0.33*p]\n",
    "        #     ints2 = set(np.array(list(ints2[0]) + other))\n",
    "        #     if len(ints2):\n",
    "        #         # if there is a peak within +/- 33%, check the power of it, choose highest\n",
    "        #         max_peak = xpeaks[[var * glbl_power][0].tolist().index(np.max([[var * glbl_power][0][i] for i in ints2]))]\n",
    "        #         new_xpeaks[peak] = max_peak\n",
    "        # xpeaks = sorted(set(new_xpeaks.values()))\n",
    "        \n",
    "        assert len(xpeaks) == len(powers)\n",
    "        print(xpeaks)\n",
    "        sig_peaks_dict[subject] = xpeaks\n",
    "        sig_power_dict[subject] = powers\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010bccbc-9316-4995-825a-440ffe6c004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file1 = open('sig_peaks_hr_dict.pkl', 'wb')\n",
    "# pickle.dump(sig_peaks_dict, file1)\n",
    "\n",
    "# file2 = open('sig_power_hr_dict.pkl', 'wb')\n",
    "# pickle.dump(sig_power_dict, file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d1fe15-8c17-4cc3-aeb6-294bd530126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot_power_vs_hdrs(all_power_slope_vec, \n",
    "                               all_hdrs_slope_vec, \n",
    "                               subject_significance_list, \n",
    "                               corr_result):\n",
    "    \n",
    "    colors = {'significant':'green', 'not significant':'red'}\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    ax.scatter(all_power_slope_vec, \n",
    "               all_hdrs_slope_vec, \n",
    "               c = [colors[sig] for sig in subject_significance_list])\n",
    "    \n",
    "    ax.plot(all_power_slope_vec, \n",
    "            corr_result.intercept + corr_result.slope*np.asarray(all_power_slope_vec), \n",
    "            c = 'blue', \n",
    "            alpha = 0.5,\n",
    "            label = 'Correlation = ' + str(round(corr_result.rvalue, 2)))\n",
    "    \n",
    "    ax.text(x = -0.2, \n",
    "            y = 0, \n",
    "            s = 'p = ' + str(round(corr_result.pvalue, 2)), \n",
    "            weight = 'bold', \n",
    "            size = 10)\n",
    "    \n",
    "    ax.legend(loc = 1)\n",
    "    ax.grid(True)\n",
    "    ax.set_xlabel('Slope of Circadian HR Rhythm Power Over 12 Weeks')\n",
    "    ax.set_ylabel('Slope of HDRS Over 12 Weeks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc5c4c-2092-497d-b40b-e4a65d22f97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linregress_power_and_hrds(df_cwt,\n",
    "                              freq, \n",
    "                            subject,\n",
    "                            HDRS_array):\n",
    "    \n",
    "    HDRS_array = zscore(HDRS_array)\n",
    "    power_array = df_cwt.resample('D').mean()[freq]\n",
    "    \n",
    "    hdrs_slope, hdrs_intercept, _, _, _ = linregress(HDRS_array, np.arange(len(HDRS_array)))\n",
    "    \n",
    "    power_slope, power_intercept, _, _, _ = linregress(power_array, np.arange(len(power_array)))\n",
    "                \n",
    "    return hdrs_slope, hdrs_intercept, power_slope, power_intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0853f06-3494-4d4b-9696-3142abc3f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file1 = open('sig_peaks_eda_dict.pkl', 'rb')\n",
    "# sig_peaks_dict = pickle.load(file1)\n",
    "\n",
    "# file2 = open('sig_power_eda_dict.pkl', 'rb')\n",
    "# sig_power_dict = pickle.load(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9c1579-3df5-4b1f-8e75-779b13cbfc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hdrs_slope_vec, all_power_slope_vec, subject_significance_list = [], [], []\n",
    "test_freq = 1.0*24\n",
    "\n",
    "for file in os.listdir(SAVE_CWT_FOLDER):\n",
    "    \n",
    "    filename = os.fsdecode(file)\n",
    "    \n",
    "    if filename.endswith(\"cwt.h5\"):\n",
    "        \n",
    "        subject = filename.split('_')[0]\n",
    "        # print(subject)\n",
    "        eda_filepath = SAVE_CWT_FOLDER+filename\n",
    "        df_cwt = pd.read_hdf(eda_filepath, 'df')\n",
    "        \n",
    "        HDRS_file_name = LABEL_FOLDER_PATH + subject + '_HDRS.csv'\n",
    "        HDRS_df = pd.read_csv(HDRS_file_name)\n",
    "        HDRS_array = HDRS_df[HDRS_variant].interpolate().to_numpy() # in case of missed HDRS assessment       \n",
    "\n",
    "        hdrs_slope, hdrs_intercept, power_slope, power_intercept = linregress_power_and_hrds(df_cwt, \n",
    "                                                                                            freq = str(test_freq),\n",
    "                                                                                            subject = subject,\n",
    "                                                                                            HDRS_array = HDRS_array)\n",
    "\n",
    "        all_hdrs_slope_vec.append(hdrs_slope)\n",
    "        all_power_slope_vec.append(power_slope)\n",
    "        \n",
    "        if test_freq in sig_peaks_dict[subject]:\n",
    "            subject_significance_list.append('significant')\n",
    "        else:\n",
    "            subject_significance_list.append('not significant')\n",
    "\n",
    "\n",
    "\n",
    "result = linregress(all_power_slope_vec, all_hdrs_slope_vec)\n",
    "\n",
    "scatter_plot_power_vs_hdrs(all_power_slope_vec, \n",
    "                           all_hdrs_slope_vec, \n",
    "                           subject_significance_list, \n",
    "                           result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90b9214-ed36-4a1c-9265-4849ec8bd313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_peak_power_dist(sig_peaks_dict, sig_power_dict):\n",
    "    \n",
    "    sig_peaks_list = []\n",
    "    avg_power_dict = {}\n",
    "    \n",
    "    for subject, peaks in sig_peaks_dict.items():\n",
    "        \n",
    "        sig_peaks_list += peaks\n",
    "\n",
    "        for idx, p in enumerate(peaks):\n",
    "            rounded_peak = round(p, 0)\n",
    "            if rounded_peak in avg_power_dict.keys():\n",
    "                avg_power_dict[rounded_peak].append(p)\n",
    "            else:\n",
    "                avg_power_dict[rounded_peak] = [p]\n",
    "    \n",
    "    rounded_peak_vec = list(avg_power_dict.keys())\n",
    "    avg_power_vec = []\n",
    "    sd_power_vec = []\n",
    "    for rounded_peak in rounded_peak_vec:\n",
    "        avg_power_vec.append(np.mean(avg_power_dict[rounded_peak]))\n",
    "        sd_power_vec.append(np.std(avg_power_dict[rounded_peak]))\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n",
    "    # make a little extra space between the subplots\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    ax1.hist(sig_peaks_list, bins = 'auto', color = 'red', alpha = 0.5)\n",
    "    ax1.set_xlabel('Period [Hour]')\n",
    "    ax1.set_ylabel('Number of Subjects')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    vline_labels = ['1 Day', '7 Days', '30 Days']\n",
    "    vline_positions = [24, 24*7, 24*30]\n",
    "    for label, position in zip(vline_labels, vline_positions):\n",
    "        ax1.axvline(position, color = 'green', label = label)\n",
    "    ax1.legend(loc=1)\n",
    "    ax1.set_title('Number of subjects with significant peaks at different periods')\n",
    "    \n",
    "#     ax2.errorbar(rounded_peak_vec, \n",
    "#                  avg_power_vec, \n",
    "#                  fmt = 'o',\n",
    "#                  yerr = sd_power_vec, \n",
    "#                  capsize = 5,\n",
    "#                  lw = 1,\n",
    "#                  label ='1 SD')\n",
    "    \n",
    "#     ax2.set_xlabel('Period [Hour]')\n",
    "#     ax2.set_ylabel('Average Power')\n",
    "\n",
    "#     for label, position in zip(vline_labels, vline_positions):\n",
    "#         ax2.axvline(position, color = 'green', label = label)\n",
    "#     ax2.legend(loc=1)\n",
    "#     ax2.set_title('Average power of significant peaks at different periods over all subjects')\n",
    "#     ax2.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a6956b-0789-4965-afc9-224ce5c289a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_peak_power_dist(sig_peaks_dict, sig_power_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad597dc2-10df-4271-95c3-bff32a4a1e72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensecode",
   "language": "python",
   "name": "sensecode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
